{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "premium",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gn04oEp0_lRK",
        "outputId": "7c83a817-8ef6-46b3-b800-8650f9a62a5a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting opendatasets\n",
            "  Downloading opendatasets-0.1.22-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.8/dist-packages (from opendatasets) (1.5.12)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from opendatasets) (4.64.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from opendatasets) (7.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from kaggle->opendatasets) (2.25.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.8/dist-packages (from kaggle->opendatasets) (2022.12.7)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.8/dist-packages (from kaggle->opendatasets) (1.24.3)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.8/dist-packages (from kaggle->opendatasets) (1.15.0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.8/dist-packages (from kaggle->opendatasets) (8.0.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.8/dist-packages (from kaggle->opendatasets) (2.8.2)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.8/dist-packages (from python-slugify->kaggle->opendatasets) (1.3)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->kaggle->opendatasets) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->kaggle->opendatasets) (2.10)\n",
            "Installing collected packages: opendatasets\n",
            "Successfully installed opendatasets-0.1.22\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (1.3.5)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.8/dist-packages (from pandas) (1.21.6)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install opendatasets\n",
        "!pip install pandas"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "WqNnYR70wWZz",
        "outputId": "0ab62c7f-6098-4fd7-a316-2db5366359df",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import os\n",
        "from glob import glob\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import stats\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.optim as optim \n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "import random\n",
        "from skimage.transform import resize\n",
        "import cv2\n",
        "import torchvision"
      ],
      "metadata": {
        "id": "3Eoechuowmci"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 72\n",
        "torch.manual_seed(seed)         \n",
        "torch.cuda.manual_seed(seed)    "
      ],
      "metadata": {
        "id": "RpkCG5tQwYso"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here you have to"
      ],
      "metadata": {
        "id": "V-3TyQ_VsRTS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import opendatasets as od\n",
        "import pandas\n",
        "\n",
        "od.download(\n",
        "\t\"https://www.kaggle.com/datasets/nih-chest-xrays/data\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mib2mfvOAxrE",
        "outputId": "47a07dfb-ad5b-47ca-ce25-380c3ea14159"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data.zip to ./data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 42.0G/42.0G [34:32<00:00, 21.8MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#☕ **Loading the dataset & preprocessing the data**\n",
        "\n",
        "let's get the X-ray scans and the lables from the NIH dataset, analyze it and preprocess it"
      ],
      "metadata": {
        "id": "bPNGd20mU6qV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_xray_df = pd.read_csv('data/Data_Entry_2017.csv')\n",
        "all_image_paths = {os.path.basename(x): x for x in \n",
        "                   glob(os.path.join('data', 'images*', '*', '*.png'))}\n",
        "print('Scans found:', len(all_image_paths), ', Total Headers', all_xray_df.shape[0])\n",
        "all_xray_df['path'] = all_xray_df['Image Index'].map(all_image_paths.get)\n",
        "#all_xray_df['Patient Age'] = all_xray_df['Patient Age'].map(lambda x: int(x[:-1]))\n",
        "all_xray_df.sample(3)"
      ],
      "metadata": {
        "id": "OiX77p8MB7KC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  standardizing the Patient Age\n",
        " \n",
        " as said, we would like to standardize the age such that  $\\mu = 0$ , $\\sigma = 1$\n",
        " but first, let's verify that the distribution even resambles a normal one:"
      ],
      "metadata": {
        "id": "LVkWe6boCEPY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mu = np.mean(all_xray_df['Patient Age'])\n",
        "sigma = np.std(all_xray_df['Patient Age'])\n",
        "print(sigma)\n",
        "\n",
        "\n",
        "plt.hist(all_xray_df['Patient Age'], bins=500)\n",
        "\n",
        "plt.gca().set(title='age Histogram', ylabel='Frequency',xlabel = 'age');\n",
        "plt.xlim([0, 100])\n",
        "\n",
        "x_data = np.arange(0, 100, 0.001)\n",
        "\n",
        "## y-axis as the gaussian\n",
        "y_data = stats.norm.pdf(x_data, mu, sigma)*len(all_xray_df['Patient Age'])\n",
        "\n",
        "\n",
        "plt.plot(x_data,y_data)\n",
        "plt.legend([\"gaussian curve\",\"age histogram\"])"
      ],
      "metadata": {
        "id": "rTtP57xWCHIT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "seems pretty close, let us standardize it"
      ],
      "metadata": {
        "id": "eQHeX2WgCOi9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'𝜇 = {mu},\\t 𝜎 = {sigma}')\n",
        "\n",
        "all_xray_df['Patient Age'] = (all_xray_df['Patient Age'])/ sigma - mu/sigma\n"
      ],
      "metadata": {
        "id": "EAkdg6QBCPyi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "now compute and display the new $\\mu $ and $\\sigma $\n"
      ],
      "metadata": {
        "id": "pmPfMnfxCaow"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mu = np.mean(all_xray_df['Patient Age'])\n",
        "sigma = np.std(all_xray_df['Patient Age'])\n",
        "\n",
        "print(f'𝜇 = {mu},\\t 𝜎 = {sigma}')"
      ],
      "metadata": {
        "id": "xcG_HWKECV3G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "let's view the difference in our dataset"
      ],
      "metadata": {
        "id": "iL3PLKpTCtyN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_xray_df.sample(3)\n"
      ],
      "metadata": {
        "id": "-KwM7jnvC0J1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Z1cAFeBoRArD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Adjusting the output\n",
        "Here we take the labels and make them into a more clear format. what's most important for us right now is to devide our label values into 2: cardiomegali diagnosed or \"No finding\"."
      ],
      "metadata": {
        "id": "Lwy-q5loC7eS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_xray_df['Finding Labels'] = [x.split(\"|\") for x in all_xray_df['Finding Labels']]\n"
      ],
      "metadata": {
        "id": "8Y_lEh_qDGcH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_xray_df['Cardiomegaly'] = [('Cardiomegaly' in x)*1 for x in all_xray_df['Finding Labels']]\n",
        "all_xray_df['No Finding'] = [('No Finding' in x)*1 for x in all_xray_df['Finding Labels']]\n",
        "\n",
        "print(f\"number of patients diagnosed with cardiomegaly: {np.sum(all_xray_df['Cardiomegaly'])}\")\n",
        "print(f\"number of patients diagnosed with No Finding: {np.sum(all_xray_df['No Finding'])}\")"
      ],
      "metadata": {
        "id": "4l_cKJdKDJqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with_cardio = [plt.imread(all_xray_df['path'][i]) for i in range(4)]\n",
        "# plt.imshow(with_cardio,cmap = 'bone')\n",
        "\n",
        "\n",
        "fig, m_axs = plt.subplots(2, 2,figsize = (16, 16))\n",
        "fig.suptitle('first 4 patients')\n",
        "\n",
        "for i, ax in enumerate(m_axs.flatten()):\n",
        "    \n",
        "    print(all_xray_df['Cardiomegaly'][i])\n",
        "    ax.imshow(with_cardio[i],cmap = 'bone')\n",
        "    ax.set_title(f\"{'NOT'*(not all_xray_df['Cardiomegaly'][i])} diagnozed with Cardiomegaly\")\n",
        "\n"
      ],
      "metadata": {
        "id": "kWjJmHY1DN5u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s = set()\n",
        "for p in all_xray_df['Finding Labels']:\n",
        "  for x in p:\n",
        "    #print(x)\n",
        "    s.add(x)\n",
        "\n",
        "print(s)"
      ],
      "metadata": {
        "id": "-YerB-QIwqYa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corr_df = pd.DataFrame()\n",
        "\n",
        "corr_df['Infiltration'] = [('Infiltration' in x)*1 for x in all_xray_df['Finding Labels']]\n",
        "corr_df['Cardiomegaly'] = [('Cardiomegaly' in x)*1 for x in all_xray_df['Finding Labels']]\n",
        "corr_df['Nodule'      ] = [('Nodule' in x)*1 for x in all_xray_df['Finding Labels']]\n",
        "corr_df['Pneumonia'   ] = [('Pneumonia' in x)*1 for x in all_xray_df['Finding Labels']]\n",
        "corr_df['Consolidation'] = [('Consolidation' in x)*1 for x in all_xray_df['Finding Labels']]\n",
        "#corr_df['No Finding'  ] = [('No Finding' in x)*1 for x in all_xray_df['Finding Labels']]\n",
        "corr_df['Hernia'      ] = [('Hernia' in x)*1 for x in all_xray_df['Finding Labels']]\n",
        "corr_df['Pleural_Thickening'] = [('Pleural_Thickening' in x)*1 for x in all_xray_df['Finding Labels']]\n",
        "corr_df['Fibrosis'    ] = [('Fibrosis' in x)*1 for x in all_xray_df['Finding Labels']]\n",
        "corr_df['Atelectasis' ] = [('Atelectasis' in x)*1 for x in all_xray_df['Finding Labels']]\n",
        "corr_df['Pneumothorax'] = [('Pneumothorax' in x)*1 for x in all_xray_df['Finding Labels']]\n",
        "corr_df['Emphysema'   ] = [('Emphysema' in x)*1 for x in all_xray_df['Finding Labels']]\n",
        "corr_df['Edema'       ] = [('Edema' in x)*1 for x in all_xray_df['Finding Labels']]\n",
        "corr_df['Mass'        ] = [('Mass' in x)*1 for x in all_xray_df['Finding Labels']]\n",
        "corr_df['Effusion'    ] = [('Effusion' in x)*1 for x in all_xray_df['Finding Labels']]\n",
        "\n",
        "for pathologie in s:\n",
        "  print(f'corr_df[\\'{pathologie}\\'] = [(\\'{pathologie}\\' in x)*1 for x in all_xray_df[\\'Finding Labels\\']]')\n"
      ],
      "metadata": {
        "id": "89red_N40eNh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "o5t2H-sI10VB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation matrix\n",
        "def plotCorrelationMatrix(df, graphWidth):\n",
        "    filename = \"a\"\n",
        "    df = df.dropna('columns') # drop columns with NaN\n",
        "    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n",
        "    if df.shape[1] < 2:\n",
        "        print(f'No correlation plots shown: The number of non-NaN or constant columns ({df.shape[1]}) is less than 2')\n",
        "        return\n",
        "    corr = df.corr()\n",
        "    plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')\n",
        "    corrMat = plt.matshow(corr, fignum = 1)\n",
        "    plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)\n",
        "    plt.yticks(range(len(corr.columns)), corr.columns)\n",
        "    plt.gca().xaxis.tick_bottom()\n",
        "    plt.colorbar(corrMat)\n",
        "    plt.title(f'Correlation Matrix for {filename}', fontsize=15)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "df = pd.DataFrame()\n",
        "plotCorrelationMatrix(corr_df,14)"
      ],
      "metadata": {
        "id": "aB1j9DIcspBB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "wwgvG_2cDb-g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "we want to resize the image so we check the image features for (200,200)"
      ],
      "metadata": {
        "id": "z1ZpvMMFpY73"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img = plt.imread(all_xray_df['path'][0])\n",
        "img=resize(img,(200,200))\n",
        "plt.imshow(img)"
      ],
      "metadata": {
        "id": "5mTPX8eTl1Ot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_xray_df['Gender'] =  [0 if g =='M' else 1 for g in all_xray_df['Patient Gender']]\n",
        "all_xray_df['position'] =  [0 if g =='AP' else 1 for g in all_xray_df['View Position']]\n",
        "all_xray_df.sample(3)"
      ],
      "metadata": {
        "id": "diuVF_9tJOZ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building the balanced Cardiomegali dataset\n",
        "\n",
        "*   we take positive cases and for the sake of balancing the data, we take a close amount for the negative cases.\n",
        "\n",
        "*   as a balnce vs. dataset size trade off,  we take $ξ$ more negative cases to enlarge the total size. \n",
        "\n"
      ],
      "metadata": {
        "id": "BDmboOFcNEkJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mask = all_xray_df['Cardiomegaly'] == 1\n",
        "positive = all_xray_df.loc[mask]\n",
        "positive = positive.reset_index(drop=True)\n",
        "\n",
        "neg_mask = all_xray_df['No Finding'] == 1\n",
        "negative = all_xray_df.loc[neg_mask]\n",
        "negative = negative.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "\n",
        "#chosen_idx = np.random.choice(len(negative), replace = False, size = int(n*1.2))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JsCKYfQPDbrP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Split the Data to Train, Valid and Test\n",
        "\n",
        "\n",
        "\n",
        "*   positive - \"cardiomegaly\"\n",
        "\n",
        "*   negative - \"No findings\"\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "R3jMjZm1TdIu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TRAINSIZE = 0.86\n",
        "VALSIZE = 0.93\n",
        "\n",
        "\n",
        "p =len(positive)\n",
        "n = len(negative)\n",
        "N = n+p\n",
        "\n",
        "\n",
        "data = pd.concat([positive,negative]).sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "\n",
        "pos_train, pos_test, pos_val = positive.iloc[:int(TRAINSIZE*p)], positive.iloc[int(TRAINSIZE*p):int(VALSIZE*p)],positive.iloc[int(VALSIZE*p):]\n",
        "neg_train, neg_test, neg_val = negative.iloc[:int(TRAINSIZE*n)], negative.iloc[int(TRAINSIZE*n):int(VALSIZE*n)],negative.iloc[int(VALSIZE*n):n]\n",
        "data_train, data_test, data_val = data.iloc[:int(TRAINSIZE*N)], data.iloc[int(TRAINSIZE*N):int(VALSIZE*N)],data.iloc[int(VALSIZE*N):]\n",
        "\n",
        "data_train = data_train.reset_index(drop=True)\n",
        "pos_train = pos_train.reset_index(drop=True)\n",
        "neg_train = neg_train.reset_index(drop=True)\n",
        "\n",
        "data_test = data_test.reset_index(drop=True)\n",
        "pos_test = pos_test.reset_index(drop=True)\n",
        "neg_test = neg_test.reset_index(drop=True)\n",
        "\n",
        "data_val = data_val.reset_index(drop=True)\n",
        "pos_val = pos_val.reset_index(drop=True)\n",
        "neg_val = neg_val.reset_index(drop=True)\n",
        "pos_val\n"
      ],
      "metadata": {
        "id": "6Hw4CVW9S-hj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transforming the Dataset to Tensors\n",
        "\n",
        "Our data is stored in collabs hard disc what cause high runtime, we will write a function that will store the train, val, test images in tensors."
      ],
      "metadata": {
        "id": "CZ2iZXl6pz7P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def collect_tensor(data,H,W):\n",
        "  n=len(data)\n",
        "  data_array = torch.zeros((n,H,W))\n",
        "  variables = torch.zeros((n,3))\n",
        "\n",
        "  for j in range(n):\n",
        "    if plt.imread(data['path'][j]).shape==(1024,1024):\n",
        "      data_array[j] = torch.tensor(resize(plt.imread(data['path'][j])[144:960,80:960],(H,W)))   \n",
        "    else:\n",
        "      data_array[j] = torch.tensor(resize(plt.imread(data['path'][j])[144:960,80:960,0],(H,W)))\n",
        "    \n",
        "    \n",
        "    variables[j,0] = data['Patient Age'][j]\n",
        "    variables[j,1] = data['Gender'][j]\n",
        "    variables[j,2] = data['position'][j]\n",
        "\n",
        "  return data_array, variables\n",
        "     \n"
      ],
      "metadata": {
        "id": "Brlm9IOrrFs_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "H=224\n",
        "W=224\n",
        "\n",
        "pos_train_arr , vars_pos_train_arr = collect_tensor(pos_train,H,W)\n",
        "\n",
        "pos_test_arr , vars_pos_test_arr = collect_tensor(pos_test,H,W)\n",
        "\n",
        "pos_val_arr  , vars_pos_val_arr = collect_tensor(pos_val,H,W)\n",
        "\n",
        "neg_train_arr  , vars_neg_train_arr = collect_tensor(neg_train[:2500],H,W)\n",
        "\n",
        "neg_test_arr ,vars_neg_test_arr = collect_tensor(neg_test[:300],H,W)\n",
        "\n",
        "neg_val_arr  ,vars_neg_val_arr = collect_tensor(neg_val[:300],H,W)\n"
      ],
      "metadata": {
        "id": "vkXvT21jt-Ti"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For testing the data we will try to look for different crops for the image, and different resizes to decide what transformation our good for the data."
      ],
      "metadata": {
        "id": "sbqdGl0xqQSv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "m, n =20, 5 \n",
        "fig, axs = plt.subplots(m, n, figsize= (n*3,m*4))\n",
        "for i in range(100):\n",
        "  plt.figure()\n",
        "  axs[i//n,i%n].imshow(resize(plt.imread(pos_train['path'][i])[144:960,80:960],(200,200)))\n",
        "\n",
        "  "
      ],
      "metadata": {
        "id": "776aolL2x-ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = cv2.cvtColor(resize(plt.imread(pos_train['path'][0])[144:960,80:960],(200,200)),cv2.COLOR_GRAY2BGR)"
      ],
      "metadata": {
        "id": "-DG3NXkJ9OcX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ????\n",
        "\n",
        "We will write a function which will generate positive/negative tensor batches for the accuracy check function."
      ],
      "metadata": {
        "id": "iUNgVkp7yqWC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_batch_tensor(data):\n",
        "   n=len(data)\n",
        "   xt = torch.zeros((n,224,224))\n",
        "   vars = torch.zeros((n,3))\n",
        "   for j in range(n):  \n",
        "      xt[j]=data[j]\n",
        "      #vars[j][0]=data[j][1]\n",
        "      #vars[j][1]=data[j][2]\n",
        "      #vars[j][2]=data[j][3]\n",
        "      \n",
        "   xt=xt.reshape(n,224,224,1)\n",
        "   xt=torch.cat((xt,xt,xt),3)\n",
        "   xt=xt.reshape(n,3,224,224)\n",
        "   \n",
        "\n",
        "   return xt,vars          \n",
        "  "
      ],
      "metadata": {
        "id": "VVE3EhY2ni9J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "vectorizing the gender column\n",
        "\n",
        "### bold text **TODO** we need to figure out how to demonstrate / prove correlation between gender and cardio.\n",
        "\n",
        "if there is none, remove the column "
      ],
      "metadata": {
        "id": "mL8B6XTzYJhX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fu_mean = np.mean(all_xray_df['Follow-up #'])\n",
        "fu_mean_cardio = np.mean(all_xray_df.loc[all_xray_df['Cardiomegaly'] == True]['Follow-up #'])\n",
        "\n",
        "print(f'followup mean in dataset:{fu_mean}\\nfollowup mean for diagnosed patients:{fu_mean_cardio}\\n')"
      ],
      "metadata": {
        "id": "sjX1TNZDb5oy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# 🕸 **Network architecture**\n",
        "\n",
        "Now we will build the architecture of the model.\n",
        "\n",
        "important to mention that we used some image shapes: 1024X1024, 880X816, 200X200.\n",
        "\n",
        "**BLOCK 1:**\n",
        "\n",
        "\n",
        "\n",
        "*   Convolutional layer (nn.Conv2D(in_channels, num_hidden,kernel_size=(3,3), stride=(2,2)))\n",
        "\n",
        "*   Batch Normalization(num_hidden)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "*   MaxPool2d(kernel_size=2, stride=2))\n",
        "\n",
        "*   Activation Function: nn.ReLU()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**BLOCK 2:**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "*   Convolutional layer (nn.Conv2D(num_hidden, num_hidden * 2, kernel_size=(3,3), stride=(2,2))\n",
        "\n",
        "*   Activation Function: nn.ReLU()\n",
        "\n",
        "*   Batch Normalization(num_hidden * 2)\n",
        "*   MaxPool2d(kernel_size=2, stride=2))\n",
        "\n",
        "\n",
        "\n",
        "**BLOCK 3:**\n",
        "\n",
        "\n",
        "*   Convolutional layer (nn.Conv2D(num_hidden * 2, num_hidden * 4, kernel_size=(3,3), stride=(2,2))\n",
        "\n",
        "*   Activation Function: nn.ReLU()\n",
        "\n",
        "*   Batch Normalization(num_hidden * 4)\n",
        "\n",
        "\n",
        " \n",
        "**BLOCK 4:**\n",
        "\n",
        "\n",
        "*   Convolutional layer (nn.Conv2D(num_hidden * 4, num_hidden * 8, kernel_size=(3,3), stride=(2,2))\n",
        "\n",
        "*   Activation Function: nn.ReLU()\n",
        "\n",
        "\n",
        "\n",
        "*   Batch Normalization(num_hidden * 8)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**BLOCK 5:**\n",
        "\n",
        "\n",
        "*   FC layer(3 * 3 * 8 * num_hiddens,100)\n",
        "\n",
        "*   Activation Function: nn.Sigmoid()\n",
        "\n",
        "*   FC layer(100,20)\n",
        "\n",
        "*   Activation Function: nn.Sigmoid())\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**BLOCK 6:**\n",
        "\n",
        "In this chapter we add the arguments: [Age,Gender,angle]\n",
        "                           \n",
        "\n",
        "*   FC layer(20 + num_vars, 2)\n",
        "*   nn.Sigmoid()\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fAv9i6klgRHx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "input size: 1024x1024"
      ],
      "metadata": {
        "id": "8-do-TRc3dit"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "W,H = 1024,1024\n",
        "\n",
        "class CNN1(nn.Module):\n",
        "    def __init__(self, num_hiddens,num_vars):\n",
        "\n",
        "        \n",
        "        super(CNN, self).__init__()\n",
        "\n",
        "        H = 1024\n",
        "        W = 1024\n",
        "        in_channels = 1\n",
        "        self.num_hiddens = num_hiddens\n",
        "        self.block1 = nn.Sequential(nn.Conv2d(in_channels, num_hiddens, kernel_size=(3,3), stride=(2,2),padding=(1,1)),\n",
        "                                    nn.BatchNorm2d(num_hiddens),\n",
        "                                    nn.ReLU())\n",
        "        \n",
        "        self.block2 = nn.Sequential(nn.Conv2d(num_hiddens, num_hiddens*2, kernel_size=(3,3), stride=(2,2),padding=(1,1)),\n",
        "                                    nn.BatchNorm2d(num_hiddens*2),\n",
        "                                    nn.ReLU())\n",
        "\n",
        "        self.block3 = nn.Sequential(nn.Conv2d(num_hiddens*2, num_hiddens*4, kernel_size=(3,3), stride=(2,2),padding=(1,1)),\n",
        "                                    nn.BatchNorm2d(num_hiddens*4),\n",
        "                                    nn.ReLU())\n",
        "        \n",
        "        self.block4 = nn.Sequential(nn.Conv2d(num_hiddens*4, num_hiddens*8, kernel_size=(3,3), stride=(2,2),padding=(1,1)),\n",
        "                                    nn.BatchNorm2d(num_hiddens*8),\n",
        "                                    nn.ReLU())\n",
        "\n",
        "\n",
        "        self.fc_block1 = nn.Sequential(nn.Linear((W//16)*(H//16)*8*num_hiddens,100), #flatten the CNN output -> Block 5\n",
        "                                       nn.Sigmoid(),\n",
        "                                       nn.Linear(100,20),\n",
        "                                       nn.Sigmoid())\n",
        "        \n",
        "        self.fc_block2 =  nn.Sequential(nn.Linear(20 + num_vars, 10), #flatten the CNN output -> Block 6\n",
        "                                       nn.Sigmoid(),\n",
        "                                       nn.Linear(10,2),\n",
        "                                       nn.Sigmoid())\n",
        "         \n",
        "\n",
        "    def forward(self,x,vars):\n",
        "        \n",
        "        x = self.block1(x)\n",
        "        x = self.block2(x)\n",
        "        x = self.block3(x)\n",
        "        x = self.block4(x)\n",
        "        x = x.contiguous().view(-1,(W//16)*(H//16)*8*self.num_hiddens) # reshape for tensor\n",
        "        x = self.fc_block1(x)\n",
        "        x= torch.cat((x,vars),1)\n",
        "        x = self.fc_block2(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cAt7_Qpq3LHX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "k3X_VfXg3K2s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "input size: $224 𝗑 224$\n"
      ],
      "metadata": {
        "id": "93-srraEAlHe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model= CNN(3,2)\n",
        "model.forward(lst_train[0])"
      ],
      "metadata": {
        "id": "Z_3IyJM29fI6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Evaluation function**\n",
        "\n",
        "We will test the accuracy of the positive and negative separately.\n",
        "\n"
      ],
      "metadata": {
        "id": "ZRmqgPDJv-zq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path_saving  = '/content/gdrive/MyDrive/Intro_to_Deep_Learning/project/cardio'"
      ],
      "metadata": {
        "id": "EfQ9WDBNudCR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_accuracy(model, pos,neg,batch_size=50):\n",
        "    \"\"\"Compute the model accuracy on the data set. This function returns two\n",
        "    separate values: the model accuracy on the positive samples,\n",
        "    and the model accuracy on the negative samples.\n",
        "\n",
        "    Example Usage:\n",
        "\n",
        "    >>> model = CNN() # create untrained model\n",
        "    >>> pos_acc, neg_acc= get_accuracy(model, valid_data)\n",
        "    >>> false_positive = 1 - pos_acc\n",
        "    >>> false_negative = 1 - neg_acc\n",
        "    \"\"\"\n",
        "    len_pos= len(pos)\n",
        "    len_neg=len(neg)\n",
        "    model.eval()\n",
        "    acc_pos=0\n",
        "    acc_neg=0\n",
        "    H=224\n",
        "    W=224\n",
        "      \n",
        "    pos_correct = 0\n",
        "    for i in range(0, len_pos, batch_size):\n",
        "        if (i+batch_size)>len_pos:\n",
        "          break\n",
        "        acc_pos += batch_size\n",
        "        xs , var_xs = generate_batch_tensor(pos[i:i+batch_size])\n",
        "        if torch.cuda.is_available():\n",
        "            xs , var_xs = xs.cuda() , var_xs.cuda()\n",
        "        #zs = model(xs,var_xs)\n",
        "        zs = model(xs)\n",
        "        pred = zs.max(1, keepdim=True)[1] # get the index of the max logit\n",
        "        pred = pred.detach().cpu().numpy()\n",
        "        pos_correct += (pred == 1).sum()\n",
        "    \n",
        "    neg_correct = 0\n",
        "    for i in range(0, len_pos, batch_size):\n",
        "        if (i+batch_size)>len_pos:\n",
        "            break\n",
        "        acc_neg += batch_size\n",
        "        xs , var_xs = generate_batch_tensor(neg[i:i+batch_size])\n",
        "        if torch.cuda.is_available():\n",
        "            xs , var_xs = xs.cuda() , var_xs.cuda()\n",
        "        #zs = model(xs,var_xs)\n",
        "        zs = model(xs)\n",
        "        pred = zs.max(1, keepdim=True)[1] # get the index of the max logit\n",
        "        pred = pred.detach().cpu().numpy()\n",
        "        neg_correct += (pred == 0).sum()\n",
        "\n",
        "    return pos_correct/acc_pos, neg_correct/acc_neg"
      ],
      "metadata": {
        "id": "qXTjbGvG7Q2v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Training function**\n",
        "\n",
        "While training the different models we had to modify the training function multiple times.\n",
        "\n"
      ],
      "metadata": {
        "id": "NyJlsaoJITxH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def nn_train(model,\n",
        "              p_train = pos_train_arr,\n",
        "              n_train = neg_train_arr,\n",
        "              p_val = pos_val_arr,\n",
        "              n_val = neg_val_arr,\n",
        "              batch_size=10,\n",
        "              learning_rate=0.001,\n",
        "              weight_decay=0,\n",
        "              max_iters=1000,\n",
        "              checkpoint_path=None,\n",
        "              vars = False\n",
        "              ):\n",
        "#main training loop; choice of loss function; choice of optimizer\n",
        "  #print(\"hi\")\n",
        "\n",
        " \n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  criterion = criterion.cuda()\n",
        "  \n",
        "  \n",
        "  optimizer = optim.SGD(model.parameters(),\n",
        "                           lr=learning_rate,\n",
        "                           weight_decay=weight_decay,\n",
        "                           momentum=0.9)\n",
        "  \n",
        "  iters, losses = [], []\n",
        "  iters_sub, train_accs_pos , train_accs_neg, val_accs_pos, val_accs_neg  = [], [] ,[],[],[]\n",
        "  \n",
        "  len_data= min(len(p_train),len(n_train))\n",
        "  acc_len_pos=0\n",
        "  acc_len_neg=0\n",
        "  H=224\n",
        "  W=224\n",
        "  n = 1501 # the number of iterations\n",
        "  while n < max_iters:\n",
        "      for i in range(0, len_data, batch_size):\n",
        "            if (i + batch_size) > len_data:\n",
        "                break\n",
        "            \n",
        "            model.train()\n",
        "            \n",
        "            #in each iteration, take batch_size / 2 positive samples and batch_size / 2 negative samples as our input for this batch\n",
        "            #pos_st,neg_st = np.ones(batch_size / 2), np.zeros(batch_size / 2)\n",
        "\n",
        "            xt=torch.zeros((2*batch_size,H,W))  #define empty tensor for batch\n",
        "            pos_st,neg_st = np.ones((batch_size)), np.zeros((batch_size))  #define batch//2 positive and batch//2 negative\n",
        "            st = np.concatenate((pos_st,neg_st),axis=0)\n",
        "            st = np.random.permutation(st)  #permutate the positive and negative outputs\n",
        "            st_or = torch.zeros((2*batch_size,2)) #define output tensor -  negative : [1,0], positive : [0,1] \n",
        "            vars = torch.zeros((2*batch_size,3)) # define tensor for variables : [Age, gender , Angle]\n",
        "            \n",
        "            #building the batch input tensors\n",
        "            \n",
        "            pos_idx=i\n",
        "            neg_idx=i\n",
        "            j=-1\n",
        "                     \n",
        "            while( pos_idx+neg_idx<2*i+2*batch_size):  \n",
        "              j+=1 \n",
        "              if st[j]:\n",
        "                xt[j]=p_train[pos_idx]\n",
        "                st_or[j,1]=1\n",
        "                #vars[j][0]=p_train[pos_idx]\n",
        "                #vars[j][1]=p_train[pos_idx]\n",
        "                #vars[j][2]=p_train[pos_idx]\n",
        "                pos_idx+=1\n",
        "\n",
        "              else:\n",
        "                xt[j]=n_train[neg_idx]\n",
        "                st_or[j,0]=1\n",
        "                #vars[j][0]=n_train[neg_idx]\n",
        "                #vars[j][1]=n_train[neg_idx]\n",
        "                #vars[j][2]=n_train[neg_idx]\n",
        "                neg_idx+=1\n",
        "              \n",
        "          \n",
        "            \n",
        "            #conversion from numpy arrays to PyTorch tensors, making sure that the input has dimensions  N×C×H×W  (known as NCHW tensor)\n",
        "            #.. where  N  is the number of images batch size,  C  is the number of channels,  H  is the height of the image, and  W  is the width of the image.\n",
        "            \n",
        "\n",
        "            xt=xt.reshape(2*batch_size,H,W,1)\n",
        "            xt = torch.cat((xt,xt,xt),3)  #GRAYSCALE to RGB for training the resnet18,VGG16 models\n",
        "            xt=xt.reshape(2*batch_size,3,H,W)\n",
        "            \n",
        "    \n",
        "            if torch.cuda.is_available():\n",
        "                xt , vars , st = xt.cuda() , vars.cuda() , st_or.cuda()\n",
        "            \n",
        "            #zs = model(xt,vars)  # forwarding the model\n",
        "            zs = model(xt)\n",
        "            #print('zs=', zs.shape, zs)  \n",
        "            optimizer.zero_grad()\n",
        "            loss = criterion(zs, st)\n",
        "            loss.backward()\n",
        "            losses.append(float(loss)/batch_size) \n",
        "            optimizer.step()\n",
        "            \n",
        "\n",
        "            #after every epoch, report the accuracies for the training set and validation set\n",
        "            if n % 100==0:\n",
        "                iters_sub.append(n)\n",
        "                train_cost = float(loss.detach().cpu().numpy())\n",
        "                train_acc_pos,train_acc_neg  = get_accuracy(model, p_train[:300],n_train,batch_size)\n",
        "                train_accs_pos.append(train_acc_pos)\n",
        "                train_accs_neg.append(train_acc_neg)\n",
        "                val_acc_pos , val_acc_neg = get_accuracy(model, p_val,n_val,batch_size)\n",
        "                val_accs_pos.append(val_acc_pos)\n",
        "                val_accs_neg.append(val_acc_neg)\n",
        "                print(\"Iter %d. [Val pos Acc %.0f%%] [Val neg Acc %.0f%%] [Train pos Acc %.0f%%,Train neg Acc %.0f%%, Loss %f]\" % (n, val_acc_pos * 100,val_acc_neg * 100, train_acc_pos * 100, train_acc_neg * 100, train_cost))\n",
        "\n",
        "                if (n>500 and n % 400==0 and checkpoint_path is not None):  #save model\n",
        "                    n_path = checkpoint_path + f\"{n}\"\n",
        "                    torch.save(model.state_dict(), n_path.format(n))\n",
        "            # increment the iteration number\n",
        "            n += 1\n",
        "\n",
        "            if n > max_iters:\n",
        "              return model,losses, train_accs_pos, train_accs_neg, val_accs_pos, val_accs_neg\n",
        "        \n",
        "            \n",
        "            \n",
        "\n",
        "\n",
        "          #track the training curve information and plot the training curve\n",
        "        \n",
        "              \n",
        "  return model,losses, train_accs_pos, train_accs_neg, val_accs_pos, val_accs_neg"
      ],
      "metadata": {
        "id": "cxUWcxaHJYHc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 💪 **Training the Model: Trial & Error**"
      ],
      "metadata": {
        "id": "nD4WaLANRgPJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Sanity check**:\n",
        "\n",
        "we will try to memorize the data for 10 positive and 10 negative samples"
      ],
      "metadata": {
        "id": "AWxHCgEI1PH3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "  CNN_20 = CNN(3,2).cuda()\n",
        "else:\n",
        "  CNN_20 = CNN(3,2)\n",
        "\n",
        "history_20 = nn_train(CNN_20,pos_train=pos_train[:20],neg_train=neg_train[:20],batch_size=10,learning_rate=0.0001,\n",
        "              weight_decay=0,\n",
        "              max_iters=700,\n",
        "              checkpoint_path=None)"
      ],
      "metadata": {
        "id": "kZzUh6afe_KD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **First training**\n",
        "\n",
        "Now we will train the model for the first time. For curiosity, at first we train the images with 1024X1024 size to see how close the results we get.\n"
      ],
      "metadata": {
        "id": "PIQ-lgCWmR-F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "  CNN_cardio = CNN(3,2).cuda()\n",
        "else:\n",
        "  CNN_cardio = CNN(3,2)\n",
        "  \n",
        "\n"
      ],
      "metadata": {
        "id": "WM944o98si9B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CNN_cardio,losses, train_accs_pos, train_accs_neg, val_accs_pos, val_accs_neg  = nn_train(CNN_cardio,batch_size=10,learning_rate=0.0001,\n",
        "              weight_decay=0,\n",
        "              max_iters=4000,\n",
        "              checkpoint_path=path_saving+\"original_\")"
      ],
      "metadata": {
        "id": "X_uhYUhBtcf-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Overcoming Overfitting\n",
        "We see that we got an **overfitting** because the model memorized the training data but have a poor validation prediction. we added several munipulations to the images:\n",
        "1. we see that there is some area that have no information so we cropped the images.\n",
        "2. for the overfitting we added a max pooling layers to make model more robustic, and we shuffle the training data everytime it gets to an end to make it harder for the model to memorize it.\n",
        "\n",
        "Now we will train to give our model a second try."
      ],
      "metadata": {
        "id": "QOepVCX64isl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(CNN_cardio.state_dict(), (path_saving+\"cardio_10_iter\").format(n))"
      ],
      "metadata": {
        "id": "JjldqwVh-0Gu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class CNN2(nn.Module):\n",
        "    def __init__(self, num_hiddens,num_vars):\n",
        "\n",
        "        \n",
        "        super(CNN, self).__init__()\n",
        "\n",
        "        self.H = 224\n",
        "        self.W = 224\n",
        "        in_channels = 1\n",
        "        self.num_hiddens = num_hiddens\n",
        "        self.block1 = nn.Sequential(nn.Conv2d(in_channels, num_hiddens, kernel_size=(3,3), stride=(2,2),padding=(1,1)),\n",
        "                                    nn.ReLU(),\n",
        "                                    nn.BatchNorm2d(num_hiddens),\n",
        "                                    nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "        \n",
        "        self.block2 = nn.Sequential(nn.Conv2d(num_hiddens, num_hiddens*2, kernel_size=(3,3), stride=(2,2),padding=(1,1)),\n",
        "                                    nn.ReLU(),\n",
        "                                    nn.BatchNorm2d(num_hiddens*2),\n",
        "                                    nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "\n",
        "        self.block3 = nn.Sequential(nn.Conv2d(num_hiddens*2, num_hiddens*4, kernel_size=(3,3), stride=(2,2),padding=(1,1)),\n",
        "                                    nn.ReLU(),\n",
        "                                    nn.BatchNorm2d(num_hiddens*4))\n",
        "        \n",
        "        self.block4 = nn.Sequential(nn.Conv2d(num_hiddens*4, num_hiddens*8, kernel_size=(3,3), stride=(2,2),padding=(1,1)),\n",
        "                                    nn.ReLU(),\n",
        "                                    nn.BatchNorm2d(num_hiddens*8))\n",
        "\n",
        "\n",
        "        self.fc_block1 = nn.Sequential(nn.Linear(3*3*8*num_hiddens,100), #flatten the CNN output\n",
        "                                       nn.Sigmoid(),\n",
        "                                       nn.Linear(100,20),\n",
        "                                       nn.Sigmoid())\n",
        "        \n",
        "        self.fc_block2 =  nn.Sequential(nn.Linear(20 + num_vars, 2), #flatten the CNN output\n",
        "                                       nn.Sigmoid())\n",
        "        \n",
        "\n",
        "    def forward(self,x,vars):\n",
        "        \n",
        "        x = self.block1(x)\n",
        "        x = self.block2(x)\n",
        "        x = self.block3(x)\n",
        "        x = self.block4(x)\n",
        "        x = x.contiguous().view(-1,3*3*8*self.num_hiddens) # reshape for tensor\n",
        "\n",
        "        x = self.fc_block1(x)\n",
        "        #x= torch.cat((x,vars),1)\n",
        "        x = self.fc_block2(x)\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "WkpR6yFEgqU7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "  CNN_cardio2 = CNN(3,2).cuda()\n",
        "else:\n",
        "  CNN_cardio2 = CNN(3,2)"
      ],
      "metadata": {
        "id": "ObhMoRfpdki9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "  CNN_cardio_shuffled = CNN(3,2).cuda()\n",
        "else:\n",
        "  CNN_cardio_shffled = CNN(3,2)\n",
        "  \n"
      ],
      "metadata": {
        "id": "Hm1_a2f7SxSa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CNN_cardio_shuffled,losses5, train_accs_pos5, train_accs_neg5, val_accs_pos5, val_accs_neg5  = nn_train(CNN_cardio_shuffled,batch_size=10,learning_rate=0.0001,\n",
        "              weight_decay=0,\n",
        "              max_iters=2500,\n",
        "              checkpoint_path=path_saving+\"_shffled\")"
      ],
      "metadata": {
        "id": "3ch6eAw_S9CC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CNN_cardio_shuffled,losses6, train_accs_pos6, train_accs_neg6, val_accs_pos6, val_accs_neg6  = nn_train(CNN_cardio_shuffled,batch_size=10,learning_rate=0.0003,\n",
        "              weight_decay=0,\n",
        "              max_iters=5000,\n",
        "              checkpoint_path=path_saving+\"_shffled\")"
      ],
      "metadata": {
        "id": "dkAK9mPUJOUY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CNN_cardio_shuffled,losses7, train_accs_pos7, train_accs_neg7, val_accs_pos7, val_accs_neg7  = nn_train(CNN_cardio_shuffled,batch_size=10,learning_rate=0.0005,\n",
        "              weight_decay=0,\n",
        "              max_iters=7500,\n",
        "              checkpoint_path=path_saving+\"_shffled\")"
      ],
      "metadata": {
        "id": "YPpHTQsdqJP2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## First attempt analysis\n",
        "\n",
        "we got to an average of **74%** of accuracy and now wee see that its hard for the model to memorize the data.\n"
      ],
      "metadata": {
        "id": "ywxWPlUa3rKF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "we tried to train the model to predict \"with cardiomegaly\" vs \"without cardiomegaly\" but there is some correlation between other diseases that make it harder for the model to learn it, so we will try to change the prediction to \"with cardiomegaly\" vs \"no findings\""
      ],
      "metadata": {
        "id": "gwWC1joW6HPz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy.core.function_base import linspace\n",
        "\n",
        "def plot_analisys(model, history)\n",
        "  plt.figure()\n",
        "  \n",
        "  # devide to sections\n",
        "  res_train_accs_pos,res_train_accs_neg, res_val_accs_pos, res_val_accs_neg = history\n",
        "  N = range(len(res_losses))\n",
        "\n",
        "  fig, axs = plt.subplots(5,figsize=(15, 15))\n",
        "\n",
        "  axs[0].plot(N, [30*res_losses[i] for i in N])\n",
        "  axs[0].set_title('losses')\n",
        "\n",
        "  N = range(len(res_train_accs_pos))\n",
        "  axs[1].plot(N, res_train_accs_pos)\n",
        "  axs[1].set_title('train positive accuracy ')\n",
        "  axs[2].plot(N, res_train_accs_neg,'tab:orange')\n",
        "  axs[2].set_title('train negative accuracy')\n",
        "  axs[3].plot(N, res_val_accs_pos, 'tab:green')\n",
        "  axs[3].set_title('validation positive accuracy')\n",
        "  axs[4].plot(N, res_val_accs_neg, 'tab:red')\n",
        "  axs[4].set_title('validation negative accuracy')\n",
        "\n",
        "  "
      ],
      "metadata": {
        "id": "1ph4tbp-uWRN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Second Attempt on a Cleaner Data"
      ],
      "metadata": {
        "id": "wlWFP363pfmS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "  NofindXcardio2 = CNN(3,3).cuda()\n",
        "else:\n",
        "  NofindXcardio2 = CNN(3,3)"
      ],
      "metadata": {
        "id": "idGs0vuDemQl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NofindXcardio2,losses2, train_accs_pos2, train_accs_neg2, val_accs_pos2, val_accs_neg2  = nn_train(NofindXcardio2,pos_train=pos_train,neg_train=neg_train[:4000],batch_size=30,learning_rate=0.0002,\n",
        "              weight_decay=0,\n",
        "              max_iters=2500,\n",
        "              checkpoint_path=path_saving+'No_findXCardio2_')"
      ],
      "metadata": {
        "id": "4NgCbHroepzo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NofindXcardio21,losses21, train_accs_pos21, train_accs_neg21, val_accs_pos21, val_accs_neg21  = nn_train(NofindXcardio2,pos_train=pos_train,neg_train=neg_train[:4000],batch_size=30,learning_rate=0.0002,\n",
        "              weight_decay=0,\n",
        "              max_iters=3200,\n",
        "              checkpoint_path=path_saving+'No_findXCardio2_')"
      ],
      "metadata": {
        "id": "gVtwSHG0OKqw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NofindXcardio22,losses22, train_accs_pos22, train_accs_neg22, val_accs_pos22, val_accs_neg22  = nn_train(NofindXcardio2,pos_train=pos_train,neg_train=neg_train[:4000],batch_size=30,learning_rate=0.0002,\n",
        "              weight_decay=0,\n",
        "              max_iters=4000,\n",
        "              checkpoint_path=path_saving+'No_findXCardio2_')"
      ],
      "metadata": {
        "id": "oOnKnyAOYnLm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2nd attempt Results & analysis\n",
        "now we got validation accuracy of 77.5%\n",
        "we will print to images that the model didn't predict right."
      ],
      "metadata": {
        "id": "klD5FiS9tABf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**False negative images**"
      ],
      "metadata": {
        "id": "ctl6PMY1tfJN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(int(0.9*p),int(0.93*p)):\n",
        "  NofindXcardio22.eval()\n",
        "  xt=torch.Tensor(plt.imread(pos_val['path'][i])[144:960,80:960]).cuda()\n",
        "  if xt.shape==(816,880):\n",
        "    xt = xt.reshape(1,1,816,880)\n",
        "  else:  \n",
        "    xt = xt[:,:,0].reshape(1,1,816,880)\n",
        "  finding=NofindXcardio22(xt,torch.Tensor([pos_val['Patient Age'][i],pos_val['Gender'][i],pos_val['position'][i]]).reshape(1,3).cuda())\n",
        "  if finding[0][0]>finding[0][1]:\n",
        "    plt.figure()\n",
        "    plt.imshow(plt.imread(pos_val['path'][i])[144:960,80:960])\n",
        "    plt.title(str(finding))"
      ],
      "metadata": {
        "id": "wN0uAlu-kquq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**False positive images**"
      ],
      "metadata": {
        "id": "R57pZ4bUtnL4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(int(0.9*n),int(0.901*n)):\n",
        "  NofindXcardio22.eval()\n",
        "  xt=torch.Tensor(plt.imread(neg_val['path'][i])[144:960,80:960]).cuda()\n",
        "  if xt.shape==(816,880):\n",
        "    xt = xt.reshape(1,1,816,880)\n",
        "  else:  \n",
        "    xt = xt[:,:,0].reshape(1,1,816,880)\n",
        "  finding=NofindXcardio22(xt,torch.Tensor([neg_val['Patient Age'][i],neg_val['Gender'][i],neg_val['position'][i]]).reshape(1,3).cuda())\n",
        "  if finding[0][0]<finding[0][1]:\n",
        "    plt.figure()\n",
        "    plt.imshow(plt.imread(neg_val['path'][i])[144:960,80:960])\n",
        "    plt.title(str(finding))"
      ],
      "metadata": {
        "id": "r65ZKFUWrVO9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "until now we tried to train the model with full sized cropped image, now we will try to resize it to shape (200,200) and train the model with the resized image size.\n",
        "unfortunatelly when we tried to train the resized images with our model we got less accurate results than in the models we already trained so we didnt insert it to the report.\n",
        "\n",
        "From now on we will try to use pretrained models: AlexNet, Resnet18 and VGG16."
      ],
      "metadata": {
        "id": "VV9fk1iF4CIr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Benchmarks\n",
        "\n",
        "here we will import real-world state of the art models thatare well known for their good detection accuracy in many test cases and some, specifically on the NIH dataset"
      ],
      "metadata": {
        "id": "nXgHrRzWsaNB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Alexnet"
      ],
      "metadata": {
        "id": "R4cDPllls8V5"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NQZ3RN5GsXLC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AlexNet(nn.Module):\n",
        "    def __init__(self, num_classes: int = 2) -> None:\n",
        "        super(AlexNet, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(1, 64, kernel_size=11, stride=4, padding=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "        )\n",
        "\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(256 * 6 * 6, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(4096, num_classes),\n",
        "        )\n",
        "        #self.block = nn.Linear(4099, num_classes)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor: #add \"vars\" argument for variables\n",
        "        x = self.features(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.classifier(x)\n",
        "        #x = torch.cat((x,vars),1)\n",
        "        #x = self.block(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "XKvP469jKOpU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "  alexnet_model = AlexNet().cuda()\n",
        "else:\n",
        "  alexnet_model = AlexNet()"
      ],
      "metadata": {
        "id": "VPSiMTjsbFrZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "alexnet_model,losses, train_accs_pos, train_accs_neg, val_accs_pos, val_accs_neg  = nn_train(alexnet_model,p_train=pos_train,n_train=neg_train[:4000],batch_size=30,learning_rate=0.0002,\n",
        "              weight_decay=0,\n",
        "              max_iters=1000,\n",
        "              checkpoint_path=path_saving+'_alexnet2_')"
      ],
      "metadata": {
        "id": "HCdP7Y-tboDg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "  alexnet_model3 = AlexNet().cuda()\n",
        "else:\n",
        "  alexnet_model3 = AlexNet()"
      ],
      "metadata": {
        "id": "129CaJrJwN5S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "alexnet_model3,alex_losses, alex_train_accs_pos, alex_train_accs_neg, alex_val_accs_pos, alex_val_accs_neg  = nn_train(alexnet_model3,p_train=pos_train,n_train=neg_train[:4000],batch_size=50,learning_rate=0.0002,\n",
        "              weight_decay=0,\n",
        "              max_iters=800,\n",
        "              checkpoint_path=path_saving+'_alexnet3_')"
      ],
      "metadata": {
        "id": "0IvAeYTvu1HS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "alexnet_model31.load_state_dict(torch.load('/content/gdrive/MyDrive/Intro_to_Deep_Learning/project/cardio_alexnet3_900'))\n",
        "alexnet_model31,alex_losses2, alex_train_accs_pos2, alex_train_accs_neg2, alex_val_accs_pos2, alex_val_accs_neg2  = nn_train(alexnet_model3,p_train=pos_train,n_train=neg_train[:4000],batch_size=50,learning_rate=0.00005,\n",
        "              weight_decay=0,\n",
        "              max_iters=1300,\n",
        "              checkpoint_path=path_saving+'_alexnet3_')"
      ],
      "metadata": {
        "id": "fCVQAIx5P-77"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "alexnet_model.load_state_dict(torch.load('/content/gdrive/MyDrive/Intro_to_Deep_Learning/project/cardio_alexnet_1200'))\n",
        "alexnet_model,losses2, train_accs_pos2, train_accs_neg2, val_accs_pos2, val_accs_neg2  = nn_train(alexnet_model,pos_train=pos_train,neg_train=neg_train[:4000],batch_size=30,learning_rate=0.00005,\n",
        "              weight_decay=0,\n",
        "              max_iters=1500,\n",
        "              checkpoint_path=path_saving+'_alexnet_')"
      ],
      "metadata": {
        "id": "zaM5gZABJcL3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "  resnet18_model = torchvision.models.resnet18(num_classes=2).cuda()\n",
        "else:\n",
        "  resnet18_model = torchvision.models.resnet18(num_classes=2)"
      ],
      "metadata": {
        "id": "CIjI8Ryt1oJe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### performance and insights\n",
        "in the architecture above we see a high accuracy results, and we hyperparameters in our model"
      ],
      "metadata": {
        "id": "BxW83VGytLd_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ResNet "
      ],
      "metadata": {
        "id": "BBhG1pzAufVd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "bJwDa01VufPr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "resnet18_model,res_losses, res_train_accs_pos, res_train_accs_neg, res_val_accs_pos, res_val_accs_neg  = nn_train(resnet18_model,p_train=pos_train,n_train=neg_train[:2500],p_val=pos_val,n_val=neg_val,batch_size=30,learning_rate=0.01,\n",
        "              weight_decay=0,\n",
        "              max_iters=1200,\n",
        "              checkpoint_path=path_saving+'_resnet2_')"
      ],
      "metadata": {
        "id": "-mCPwZmTGrxJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resnet18_model,res_losses, res_train_accs_pos, res_train_accs_neg, res_val_accs_pos, res_val_accs_neg  = nn_train(resnet18_model,p_train=pos_train,n_train=neg_train[:2400],batch_size=40,learning_rate=0.001,\n",
        "              weight_decay=0.0001,\n",
        "              max_iters=1200,\n",
        "              checkpoint_path=path_saving+'_resnet2_')"
      ],
      "metadata": {
        "id": "FqPaf9vJqI7Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resnet18_model,res_losses2, res_train_accs_pos2, res_train_accs_neg2, res_val_accs_pos2, res_val_accs_neg2  = nn_train(resnet18_model,p_train=pos_train,n_train=neg_train[:2400],batch_size=70,learning_rate=0.001,\n",
        "              weight_decay=0.0001,\n",
        "              max_iters=1600,\n",
        "              checkpoint_path=path_saving+'_resnet2_')"
      ],
      "metadata": {
        "id": "jdQ1eDyE-Xbb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resnet18_model,res_losses3, res_train_accs_pos3, res_train_accs_neg3, res_val_accs_pos3, res_val_accs_neg3  = nn_train(resnet18_model,p_train=pos_train,n_train=neg_train[:2400],batch_size=70,learning_rate=0.001,\n",
        "              weight_decay=0.0001,\n",
        "              max_iters=2300,\n",
        "              checkpoint_path=path_saving+'_resnet2_')"
      ],
      "metadata": {
        "id": "5DrnI7YZyTOY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### performance and insights"
      ],
      "metadata": {
        "id": "jfN8Bxg7w4Sk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## VGG_16"
      ],
      "metadata": {
        "id": "_LUG4nafw_8T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "  vgg16_model = torchvision.models.vgg16(num_classes=2).cuda()\n",
        "else:\n",
        "  vgg16_model = torchvision.models.vgg16(num_classes=2)"
      ],
      "metadata": {
        "id": "zgAUACqP6EMR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vgg16_model,vgg_losses, vgg_train_accs_pos, vgg_train_accs_neg, vgg_val_accs_pos, vgg_val_accs_neg  = nn_train(vgg16_model,p_train=pos_train_arr,n_train=neg_train_arr,p_val=pos_val_arr,n_val=neg_val_arr,batch_size=80,learning_rate=0.001,\n",
        "              weight_decay=0,\n",
        "              max_iters=1500,\n",
        "              checkpoint_path=path_saving+'_VGG16_')"
      ],
      "metadata": {
        "id": "1z0shh6f6AKE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vgg16_model,vgg_losses2, vgg_train_accs_pos2, vgg_train_accs_neg2, vgg_val_accs_pos2, vgg_val_accs_neg2  = nn_train(vgg16_model,p_train=pos_train_arr,n_train=neg_train_arr,p_val=pos_val_arr,n_val=neg_val_arr,batch_size=100,learning_rate=0.001,\n",
        "              weight_decay=0,\n",
        "              max_iters=4000,\n",
        "              checkpoint_path=path_saving+'_VGG16_')"
      ],
      "metadata": {
        "id": "jRQUK0q184da"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "7o26I4lVN0de"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#best_resnet18_model =torchvision.models.resnet18(num_classes=2).cuda()\n",
        "#best_resnet18_model.load_state_dict(torch.load('/content/gdrive/MyDrive/Intro_to_Deep_Learning/project/cardio_resnet_900'))\n",
        "#best_resnet18_model.eval()\n",
        "test_pos_res, test_neg_res = get_accuracy(resnet18_model,pos_val,neg_val,10)\n",
        "print(f'positive test accuracy: {test_pos_res*100}')\n",
        "print(f'negative test accuracy: {test_neg_res*100}')"
      ],
      "metadata": {
        "id": "9ClxTyLdZ5yx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Expanding Our Diagnosis "
      ],
      "metadata": {
        "id": "3Gn5u3esrNS3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "PATHOLOGY_2 = 'Effusion'\n",
        "PATHOLOGY_3 = 'Atelectasis'\n",
        "PATHOLOGY_4 = 'Pneumonia' \n",
        "\n",
        "all_xray_df[PATHOLOGY_2] = [(PATHOLOGY_2 in x)*1 for x in all_xray_df['Finding Labels']]\n",
        "all_xray_df[PATHOLOGY_3] = [(PATHOLOGY_3 in x)*1 for x in all_xray_df['Finding Labels']]\n",
        "all_xray_df[PATHOLOGY_4] = [(PATHOLOGY_4 in x)*1 for x in all_xray_df['Finding Labels']]\n",
        "\n",
        "print(f\"number of patients diagnosed with {PATHOLOGY_2}: {np.sum(all_xray_df[PATHOLOGY_2])}\")\n",
        "print(f\"number of patients diagnosed with {PATHOLOGY_3}: {np.sum(all_xray_df[PATHOLOGY_3])}\")\n",
        "print(f\"number of patients diagnosed with {PATHOLOGY_4}: {np.sum(all_xray_df[PATHOLOGY_4])}\")\n",
        "print(f\"number of patients diagnosed with No Finding: {np.sum(all_xray_df['No Finding'])}\")\n",
        "\n",
        "mask = all_xray_df['Cardiomegaly'] == 1 or all_xray_df[PATHOLOGY_2] == 1 or all_xray_df[PATHOLOGY_3] == 1 or all_xray_df[PATHOLOGY_4] == 1\n",
        "diagnosed = all_xray_df.loc[mask]\n",
        "diagnosed = diagnosed.reset_index(drop=True)\n",
        "\n",
        "neg_mask = all_xray_df['No Finding'] == 1\n",
        "negative = all_xray_df.loc[neg_mask]\n",
        "negative = negative.sample(frac=1).reset_index(n = 2000)"
      ],
      "metadata": {
        "id": "tOfloDSkhijE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TRAINSIZE = 0.86\n",
        "VALSIZE = 0.93\n",
        "\n",
        "\n",
        "p =len(positive)\n",
        "n = len(negative)\n",
        "N = n+p\n",
        "\n",
        "\n",
        "data = pd.concat([positive,negative]).sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "\n",
        "pos_train, pos_test, pos_val = positive.iloc[:int(TRAINSIZE*p)], positive.iloc[int(TRAINSIZE*p):int(VALSIZE*p)],positive.iloc[int(VALSIZE*p):]\n",
        "neg_train, neg_test, neg_val = negative.iloc[:int(TRAINSIZE*n)], negative.iloc[int(TRAINSIZE*n):int(VALSIZE*n)],negative.iloc[int(VALSIZE*n):n]\n",
        "data_train, data_test, data_val = data.iloc[:int(TRAINSIZE*N)], data.iloc[int(TRAINSIZE*N):int(VALSIZE*N)],data.iloc[int(VALSIZE*N):]\n",
        "\n",
        "data_train = data_train.reset_index(drop=True)\n",
        "pos_train = pos_train.reset_index(drop=True)\n",
        "neg_train = neg_train.reset_index(drop=True)\n",
        "\n",
        "data_test = data_test.reset_index(drop=True)\n",
        "pos_test = pos_test.reset_index(drop=True)\n",
        "neg_test = neg_test.reset_index(drop=True)\n",
        "\n",
        "data_val = data_val.reset_index(drop=True)\n",
        "pos_val = pos_val.reset_index(drop=True)\n",
        "neg_val = neg_val.reset_index(drop=True)\n",
        "pos_val\n"
      ],
      "metadata": {
        "id": "WOSlXUbQi6E2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class CNN2(nn.Module):\n",
        "    def __init__(self, num_hiddens,num_vars):\n",
        "\n",
        "        \n",
        "        super(CNN2, self).__init__()\n",
        "\n",
        "        self.H = 224\n",
        "        self.W = 224\n",
        "        in_channels = 1\n",
        "        self.num_hiddens = num_hiddens\n",
        "        self.block1 = nn.Sequential(nn.Conv2d(in_channels, num_hiddens, kernel_size=(3,3), stride=(2,2),padding=(1,1)),\n",
        "                                    nn.ReLU(),\n",
        "                                    nn.BatchNorm2d(num_hiddens),\n",
        "                                    nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "        \n",
        "        self.block2 = nn.Sequential(nn.Conv2d(num_hiddens, num_hiddens*2, kernel_size=(3,3), stride=(2,2),padding=(1,1)),\n",
        "                                    nn.ReLU(),\n",
        "                                    nn.BatchNorm2d(num_hiddens*2),\n",
        "                                    nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "\n",
        "        self.block3 = nn.Sequential(nn.Conv2d(num_hiddens*2, num_hiddens*4, kernel_size=(3,3), stride=(2,2),padding=(1,1)),\n",
        "                                    nn.ReLU(),\n",
        "                                    nn.BatchNorm2d(num_hiddens*4))\n",
        "        \n",
        "        self.block4 = nn.Sequential(nn.Conv2d(num_hiddens*4, num_hiddens*8, kernel_size=(3,3), stride=(2,2),padding=(1,1)),\n",
        "                                    nn.ReLU(),\n",
        "                                    nn.BatchNorm2d(num_hiddens*8))\n",
        "\n",
        "\n",
        "        self.fc_block1 = nn.Sequential(nn.Linear(3*3*8*num_hiddens,100), #flatten the CNN output\n",
        "                                       nn.Sigmoid(),\n",
        "                                       nn.Linear(100,20),\n",
        "                                       nn.Sigmoid())\n",
        "        \n",
        "        self.fc_block2 =  nn.Sequential(nn.Linear(20 + num_vars, 4), #flatten the CNN output\n",
        "                                       nn.Sigmoid())\n",
        "        \n",
        "\n",
        "    def forward(self,x,vars):\n",
        "        \n",
        "        x = self.block1(x)\n",
        "        x = self.block2(x)\n",
        "        x = self.block3(x)\n",
        "        x = self.block4(x)\n",
        "        x = x.contiguous().view(-1,3*3*8*self.num_hiddens) # reshape for tensor\n",
        "\n",
        "        x = self.fc_block1(x)\n",
        "        #x= torch.cat((x,vars),1)\n",
        "        x = self.fc_block2(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "PF9K77nMrLsH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def nn_train4(model,\n",
        "              x_train = pos_train_arr,\n",
        "              t_train = t_train,\n",
        "              v_train = v_train,\n",
        "              batch_size=10,\n",
        "              learning_rate=0.001,\n",
        "              weight_decay=0,\n",
        "              max_iters=1000,\n",
        "              checkpoint_path=None,\n",
        "              with_vars = False\n",
        "              ):\n",
        "  \n",
        "\n",
        "  #main training loop; choice of loss function; choice of optimizer\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  criterion = criterion.cuda()\n",
        "  \n",
        "  \n",
        "  optimizer = optim.SGD(model.parameters(),\n",
        "                           lr=learning_rate,\n",
        "                           weight_decay=weight_decay,\n",
        "                           momentum=0.9)\n",
        "  \n",
        "  iters, losses = [], []\n",
        "  acc_history = np_array  \n",
        "  len_data= len(train)\n",
        "\n",
        "  H=224\n",
        "  W=224\n",
        "  n = 1501 # the number of iterations\n",
        "  while n < max_iters:\n",
        "      for i in range(0, len_data, batch_size):\n",
        "            if (i + batch_size) > len_data:\n",
        "                break\n",
        "            \n",
        "            #set the model to 'training mode'\n",
        "            model.train()\n",
        "            #define empty tensor for batch\n",
        "            xt=torch.zeros((batch_size,H,W))      \n",
        "\n",
        "            #draw a random batch from our train dataset tensor\n",
        "            perm = torch.randperm(x_train.size(0))\n",
        "            idx = perm[:batch_size]            \n",
        "\n",
        "            xt = x_train[idx]\n",
        "            st = t_train[idx]\n",
        "            vars = v_train[idx]\n",
        "            \n",
        "            #conversion from numpy arrays to PyTorch tensors, making sure that the input has dimensions  N×C×H×W  (known as NCHW tensor)\n",
        "            #.. where  N  is the number of images batch size,  C  is the number of channels,  H  is the height of the image, and  W  is the width of the image.\n",
        "            \n",
        "\n",
        "            #optimizing performance using cuda\n",
        "            if torch.cuda.is_available():\n",
        "                xt , vars , st = xt.cuda() , vars.cuda() , st_or.cuda()\n",
        "\n",
        "            # forwarding the model\n",
        "            if with_vars:\n",
        "              zs = model(xt,vars) \n",
        "            else:\n",
        "              zs = model(xt)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss = criterion(zs, st)\n",
        "            loss.backward()\n",
        "            losses.append(float(loss)/batch_size) \n",
        "            optimizer.step()\n",
        "            \n",
        "            \n",
        "            #after every epoch, report the accuracies for the training set and validation set\n",
        "            if n % 200==0:\n",
        "                \n",
        "                # save the loss\n",
        "                train_cost = float(loss.detach().cpu().numpy())\n",
        "\n",
        "                # calculate accurecy history \n",
        "                # for each pathology we shuld have [positive accuracy,negative accuracy]\n",
        "                # positive accuracy = True Positive/Positive predictions\n",
        "                # negative accuracy = True negative/negative predictions\n",
        "\n",
        "                # acc_history = [pathology1 acc, apthology2 acc ... ] \n",
        "                train_acc_history = get_acc(acc_history,\n",
        "                                      model,\n",
        "                                      x_train,\n",
        "                                      t_train,\n",
        "                                      v_train,\n",
        "                                      )\n",
        "                val_acc_history = get_acc(acc_history,\n",
        "                                      model,\n",
        "                                      x_val,\n",
        "                                      t_val,\n",
        "                                      v_val,\n",
        "                                      )        \n",
        "                P1,P2,P3,P4 = 'Cardiomaegaly','Effusion','',''        \n",
        "                print(f'\\\n",
        "                Iter {n}. \\n{P1}:\\t[Val pos Acc {val_acc_history[0,0,-1]}%] [Val neg Acc {val_acc_history[0,1,-1]}] \\t\\t Train: [ pos Acc {train_acc_history[0,0,-1]}%, neg Acc {val_acc_history[0,0,-1]}%, Loss %{train_cost}]\\\n",
        "                          \\n{P2}:\\t[Val pos Acc {val_acc_history[1,0,-1]}%] [Val neg Acc {val_acc_history[0,1,-1]}] \\t\\t Train: [ pos Acc {train_acc_history[0,0,-1]}%, neg Acc {val_acc_history[0,1,-1]}%, Loss %{train_cost}]\\\n",
        "                          \\n{P1}:\\t[Val pos Acc {val_acc_history[0,0,-1]}%] [Val neg Acc {val_acc_history[0,1,-1]}] \\t\\t Train: [ pos Acc {train_acc_history[0,0,-1]}%, neg Acc {val_acc_history[0,1,-1]}%, Loss %{train_cost}]\\\n",
        "                          \\n{P1}:\\t[Val pos Acc {val_acc_history[0,0,-1]}%] [Val neg Acc {val_acc_history[0,1,-1]}] \\t\\t Train: [ pos Acc {train_acc_history[0,0,-1]}%, neg Acc {val_acc_history[0,1,-1]}%, Loss %{train_cost}]' \n",
        "                )\n",
        "\n",
        "                if (n>500 and n % 400==0 and checkpoint_path is not None):  #save model\n",
        "                    n_path = checkpoint_path + f\"{n}\"\n",
        "                    torch.save(model.state_dict(), n_path.format(n))\n",
        "            # increment the iteration number\n",
        "            n += 1\n",
        "\n",
        "            if n > max_iters:\n",
        "              return model,losses, train_accs_pos, train_accs_neg, val_accs_pos, val_accs_neg\n",
        "        \n",
        "            \n",
        "            \n",
        "\n",
        "\n",
        "          #track the training curve information and plot the training curve\n",
        "        \n",
        "              \n",
        "  return model,losses, train_accs_pos, train_accs_neg, val_accs_pos, val_accs_neg"
      ],
      "metadata": {
        "id": "2KiIUK0nsc2_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_acc(acc_history,model,x,t,v, with_vars = True):\n",
        "    n = 100\n",
        "                    # calculate accurecy history \n",
        "                # for each pathology we shuld have [positive accuracy,negative accuracy]\n",
        "                # positive accuracy = True Positive/Positive predictions\n",
        "                # negative accuracy = True negative/negative predictions\n",
        "\n",
        "                # acc_history = [pathology1 acc, apthology2 acc ... ] \n",
        "\n",
        "\n",
        "  #draw a random batch from our train dataset tensor\n",
        "    perm = torch.randperm(x.size(0))\n",
        "    idx = perm[:n]            \n",
        "\n",
        "    xt = x[idx]\n",
        "    st = t[idx]\n",
        "    vars = v[idx]\n",
        "\n",
        "    # forwarding the model\n",
        "    if with_vars:\n",
        "      ps = model(xt,vars) \n",
        "    else:\n",
        "      ps = model(xt)\n",
        "\n",
        "\n",
        "    # get the index of the max logit,\n",
        "    # this makes pred a sequence of 0 and 1\n",
        "    pred = ps.max(1, keepdim=True)[1] \n",
        "    pred = pred.detach().cpu().numpy()\n",
        "    pos_correct += (pred == 1).sum()\n",
        "\n",
        "    # true positives are the sum over the true values \n",
        "    pp = torch.sum(st)\n",
        "    np = torch.Tensor([n,n,n,n]) -tp\n",
        "\n",
        "    #run over all predictions\n",
        "    for i,p in enumerate(ps):\n",
        "      for j,d in enumerate(p):\n",
        "        #if the diagnosis is correct\n",
        "        if ps[i,j] == s[i,j]\n",
        "        "
      ],
      "metadata": {
        "id": "El7tzRlLZy3T"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}